from sklearn.metrics import f1_score
import torchnet as tnt
from typing import NamedTuple, Dict, Any, List, Tuple
import torch
import logging
from torch_geometric.nn import knn
import numpy as np
from torch_scatter import scatter_add
from collections import OrderedDict, defaultdict
from torch_geometric.nn.unpool import knn_interpolate
from torch_points3d.metrics.confusion_matrix import ConfusionMatrix
from torch_points3d.models.model_interface import TrackerInterface
from torch_points3d.metrics.segmentation_tracker import SegmentationTracker
from torch_points3d.metrics.base_tracker import BaseTracker, meter_value
from torch_points3d.models.panoptic.structures import PanopticResults, PanopticLabels
from torch_points3d.core.data_transform import SaveOriginalPosId, SaveLocalOriginalPosId
from torch_points_kernels import instance_iou
from .box_detection.ap import voc_ap
import time
import os
from os.path import exists, join
from torch_points3d.models.panoptic.ply import read_ply, write_ply
from sklearn.preprocessing import normalize
time_for_blockMerging=0
log = logging.getLogger(__name__)

class _Instance(NamedTuple):
    classname: str
    score: float
    indices: np.array  # type: ignore
    scan_id: int

    def iou(self, other: "_Instance") -> float:
        assert self.scan_id == other.scan_id
        intersection = float(len(np.intersect1d(other.indices, self.indices)))
        return intersection / float(len(other.indices) + len(self.indices) - intersection)

    def find_best_match(self, others: List["_Instance"]) -> Tuple[float, int]:
        ioumax = -np.inf
        best_match = -1
        for i, other in enumerate(others):
            iou = self.iou(other)
            if iou > ioumax:
                ioumax = iou
                best_match = i
        return ioumax, best_match


class InstanceAPMeter:
    def __init__(self):
        self._pred_clusters = defaultdict(list)  # {classname: List[_Instance]}
        self._gt_clusters = defaultdict(lambda: defaultdict(list))  # {classname:{scan_id: List[_Instance]}

    def add(self, pred_clusters: List[_Instance], gt_clusters: List[_Instance]):
        for instance in pred_clusters:
            self._pred_clusters[instance.classname].append(instance)
        for instance in gt_clusters:
            self._gt_clusters[instance.classname][instance.scan_id].append(instance)

    def _eval_cls(self, classname, iou_threshold):
        preds = self._pred_clusters.get(classname, [])
        allgts = self._gt_clusters.get(classname, {})
        visited = {scan_id: len(gt) * [False] for scan_id, gt in allgts.items()}
        ngt = 0
        for gts in allgts.values():
            ngt += len(gts)

        # Start with most confident first
        preds.sort(key=lambda x: x.score, reverse=True)
        tp = np.zeros(len(preds))
        fp = np.zeros(len(preds))
        for p, pred in enumerate(preds):
            scan_id = pred.scan_id
            gts = allgts.get(scan_id, [])
            if len(gts) == 0:
                fp[p] = 1
                continue

            # Find best macth in ground truth
            ioumax, best_match = pred.find_best_match(gts)

            if ioumax < iou_threshold:
                fp[p] = 1
                continue

            if visited[scan_id][best_match]:
                fp[p] = 1
            else:
                visited[scan_id][best_match] = True
                tp[p] = 1

        fp = np.cumsum(fp)
        tp = np.cumsum(tp)
        rec = tp / float(ngt)

        # avoid divide by zero in case the first detection matches a difficult
        # ground truth
        prec = tp / np.maximum(tp + fp, np.finfo(np.float64).eps)
        ap = voc_ap(rec, prec)
        return rec, prec, ap

    def eval(self, iou_threshold, processes=1):
        rec = {}
        prec = {}
        ap = {}
        for classname in self._gt_clusters.keys():
            rec[classname], prec[classname], ap[classname] = self._eval_cls(classname, iou_threshold)

        for i, classname in enumerate(self._gt_clusters.keys()):
            if classname not in self._pred_clusters:
                rec[classname] = 0
                prec[classname] = 0
                ap[classname] = 0

        return rec, prec, ap


class PanopticTracker(SegmentationTracker):
    """ Class that provides tracking of semantic segmentation as well as
    instance segmentation """

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self._metric_func = {**self._metric_func, "pos": max, "neg": min, "map": max, "cov": max, "wcov": max, "mIPre": max, "mIRec": max, "F1": max}

    def reset(self, stage="train"):
        super().reset(stage=stage)
        self._test_area = None
        self._full_vote_miou = None
        self._vote_miou = None
        self._full_confusion = None
        self._iou_per_class = {}
        self._pos = tnt.meter.AverageValueMeter()
        self._neg = tnt.meter.AverageValueMeter()
        self._acc_meter = tnt.meter.AverageValueMeter()
        self._cov =  tnt.meter.AverageValueMeter()
        self._wcov =  tnt.meter.AverageValueMeter()
        self._mIPre =  tnt.meter.AverageValueMeter()
        self._mIRec =  tnt.meter.AverageValueMeter()
        self._F1 =  tnt.meter.AverageValueMeter()
        self._ap_meter = InstanceAPMeter()
        self._scan_id_offset = 0
        self._rec: Dict[str, float] = {}
        self._ap: Dict[str, float] = {}
        self._iou_per_class = {}

    def track(
        self,
        model: TrackerInterface,
        full_res=False, 
        data=None,
        iou_threshold=0.5, #0.25,
        track_instances=True,
        min_cluster_points=10,
        **kwargs
    ):
        """ Track metrics for panoptic segmentation
        """
        #global block_count
        if not hasattr(self, "block_count"):
            self.block_count = 0
        self._iou_threshold = iou_threshold
        BaseTracker.track(self, model)
        outputs: PanopticResults = model.get_output()
        labels: PanopticLabels = model.get_labels()

        # Track semantic
        super()._compute_metrics(outputs.semantic_logits, labels.y)

        if not data:
            return
        assert data.pos.dim() == 2, "Only supports packed batches"

        # Object accuracy
        clusters, valid_c_idx = PanopticTracker._extract_clusters(outputs, min_cluster_points)
        #if not clusters:
        #    return
        predicted_labels = outputs.semantic_logits.max(1)[1]
        
        if clusters:
            if torch.max(labels.instance_labels)>0:
                tp, fp, acc = self._compute_acc(
                    clusters, predicted_labels, labels, data.batch, labels.num_instances, iou_threshold
                )
                self._pos.add(tp)
                self._neg.add(fp)
                self._acc_meter.add(acc)
                
                cov, wcov, mPre, mRec, F1 = self._compute_eval(
                    clusters, predicted_labels, labels, data.batch, labels.num_instances, self._num_classes, iou_threshold
                )
                self._cov.add(cov)
                self._wcov.add(wcov)
                self._mIPre.add(mPre)
                self._mIRec.add(mRec)
                self._F1.add(F1)
    
            # Track instances for AP
            if track_instances:
                pred_clusters = self._pred_instances_per_scan(
                    clusters, predicted_labels, outputs.cluster_scores, data.batch, self._scan_id_offset
                )
                gt_clusters = self._gt_instances_per_scan(
                    labels.instance_labels, labels.y, data.batch, self._scan_id_offset
                )
                self._ap_meter.add(pred_clusters, gt_clusters)
                self._scan_id_offset += data.batch[-1].item() + 1
            
        # Train mode or low res, nothing special to do
        if self._stage == "train" or not full_res:
            return

        # Test mode, compute votes in order to get full res predictions
        if self._test_area is None:
            self._test_area = self._dataset.test_data.clone()
            if self._test_area.y is None:
                raise ValueError("It seems that the test area data does not have labels (attribute y).")
            self._test_area.prediction_count = torch.zeros(self._test_area.y.shape[0], dtype=torch.int)
            self._test_area.votes = torch.zeros((self._test_area.y.shape[0], self._num_classes), dtype=torch.float)
            self._test_area.ins_pre = -1*torch.ones(self._test_area.y.shape[0], dtype=torch.int)
            self._test_area.max_instance = 0
            self._test_area.clusters = []
            self._test_area.scores = None
            #self._test_area.ori_pos = torch.zeros((self._test_area.pos.shape), dtype=torch.float)
            self._test_area.to(model.device)

        # Gather origin ids and check that it fits with the test set
        inputs = data if data is not None else model.get_input()
        if inputs[SaveOriginalPosId.KEY] is None:
            raise ValueError("The inputs given to the model do not have a %s attribute." % SaveOriginalPosId.KEY)

        originids = inputs[SaveOriginalPosId.KEY]
        
        #localoriginids = inputs[SaveLocalOriginalPosId.KEY]
        original_input_ids = self._dataset.test_data_spheres[self.block_count].origin_id
        if originids.dim() == 2:
            originids = originids.flatten()
        if originids.max() >= self._test_area.pos.shape[0]:
            raise ValueError("Origin ids are larger than the number of points in the original point cloud.")
        #if originids in original_input_ids:
        #    raise ValueError("Origin ids are larger than the number of points in the original point cloud.")

        # Set predictions
        self._test_area.votes[originids] += outputs.semantic_logits
        self._test_area.prediction_count[originids] += 1
        #self._test_area.ori_pos[originids] = data.coords
        
        #no scorenet
        if outputs.cluster_scores==None:
            c_scores = None
            curins_pre = self.get_cur_ins_pre_label(clusters, c_scores, predicted_labels.cpu().numpy())
            #block merging
            self._test_area.ins_pre, self._test_area.max_instance = self.block_merging(original_input_ids.cpu().numpy(), originids.cpu().numpy(), curins_pre, self._test_area.ins_pre.cpu().numpy(), self._test_area.max_instance, model.get_opt_mergeTh(), outputs, predicted_labels)
        #if with scorenet and has valid proposals after NMS and score_threshold
        elif clusters:
            c_scores = outputs.cluster_scores[valid_c_idx].cpu().numpy() #[outputs.cluster_scores[i].cpu().numpy() for i in valid_c_idx]
            curins_pre = self.get_cur_ins_pre_label(clusters, c_scores, predicted_labels.cpu().numpy())
            #block merging
            self._test_area.ins_pre, self._test_area.max_instance = self.block_merging(original_input_ids.cpu().numpy(), originids.cpu().numpy(), curins_pre, self._test_area.ins_pre.cpu().numpy(), self._test_area.max_instance, model.get_opt_mergeTh(), outputs, predicted_labels)

        #    #c_scores = np.array(c_scores)
        #    self._test_area.clusters, self._test_area.scores = self.block_merging_by_score(self._test_area.scores, self._test_area.clusters, c_scores, clusters, original_input_ids, originids)
        #    self._test_area.ins_pre = self.get_cur_ins_pre_label(self._test_area.clusters, self._test_area.scores.cpu().numpy(), self._test_area.ins_pre.cpu().numpy())
        #    self._test_area.ins_pre = torch.tensor(self._test_area.ins_pre).to(model.device)
        
        has_prediction = self._test_area.ins_pre != -1
        #instance prediction with color for subsampled cloud
        #if torch.any(has_prediction):
        #    self._dataset.to_ins_ply(
        #        self._test_area.pos[has_prediction].cpu(),
        #        self._test_area.ins_pre[has_prediction].cpu().numpy(),
        #        "Instance_subsample.ply",
        #    )
        
        self._dump_visuals_fortest(outputs,originids, valid_c_idx)
        
        self.block_count+=1
        
 
    def _dump_visuals_fortest(self, outputs,originids,valid_c_idx):
        if not os.path.exists("viz_for_test_all_proposals"):
            os.mkdir("viz_for_test_all_proposals")
        if not os.path.exists("viz_for_test_valid_proposals"):
            os.mkdir("viz_for_test_valid_proposals")
        if not hasattr(self, "spheres_count"):
            self.spheres_count = 0
        j=0
        for i, cluster in enumerate(outputs.clusters):
            semantic_prob = outputs.semantic_logits[cluster, :].softmax(dim=1)
            score_i=-1
            if outputs.cluster_scores!=None:
                score_i = outputs.cluster_scores[i]
            instance_type_i = outputs.cluster_type[i]
            predicted_semlabels = outputs.semantic_logits[cluster, :].max(1)[1]
            mask_score_i = torch.ones_like(predicted_semlabels)
            if outputs.mask_scores!=None:
                mask_score_i = outputs.mask_scores[j: cluster.shape[0]+j,:].squeeze().sigmoid()
                j = j+cluster.shape[0]
            example_name='instance_sphere{:d}_instance{:d}_score{:f}_type{:d}'.format(self.spheres_count,i,score_i,instance_type_i)
            val_name = join("viz_for_test_all_proposals", example_name)
            write_ply(val_name,
                [self._test_area.pos[originids[cluster]].detach().cpu().numpy(), 
                semantic_prob.detach().cpu().numpy(),
                predicted_semlabels.detach().cpu().numpy().astype('int32'),
                mask_score_i.detach().cpu().numpy().astype('float32'),
                self._test_area.y[originids[cluster]].detach().cpu().numpy().astype('int32')
                ],
                ['x', 'y', 'z',
                'sem_prob_1', 'sem_prob_2', 'sem_prob_3', 'sem_prob_4', 'sem_prob_5', 'sem_prob_6', 'sem_prob_7','sem_prob_8', 'sem_prob_9',
                'pre_sem_label', 'mask_score','gt_sem_label'])
            if valid_c_idx != None:
                if i in valid_c_idx:
                    val_name = join("viz_for_test_valid_proposals", example_name)
                    write_ply(val_name,
                        [self._test_area.pos[originids[cluster]].detach().cpu().numpy(), 
                        semantic_prob.detach().cpu().numpy(),
                        predicted_semlabels.detach().cpu().numpy().astype('int32'),
                        mask_score_i.detach().cpu().numpy().astype('float32'),
                        self._test_area.y[originids[cluster]].detach().cpu().numpy().astype('int32')
                        ],
                        ['x', 'y', 'z',
                        'sem_prob_1', 'sem_prob_2', 'sem_prob_3', 'sem_prob_4', 'sem_prob_5', 'sem_prob_6', 'sem_prob_7','sem_prob_8', 'sem_prob_9',
                        'pre_sem_label', 'mask_score','gt_sem_label'])
        self.spheres_count+=1
        
    def get_cur_ins_pre_label(self, clusters, cluster_scores, predicted_semlabels):
        cur_ins_pre_label = -1*np.ones_like(predicted_semlabels)
        if clusters:
            if np.all(cluster_scores!=None):
                idx=np.argsort(cluster_scores)
                for i,j in enumerate(idx):
                    cur_ins_pre_label[clusters[j].cpu().numpy()] = i
            else:
                idx=np.ones(len(clusters),dtype=np.int16)
                for i,j in enumerate(idx):
                    cur_ins_pre_label[clusters[i].cpu().numpy()] = i
        return cur_ins_pre_label
        
    def block_merging(self, originids, origin_sub_ids, pre_sub_ins, all_pre_ins, max_instance, th_merge, outputs, predicted_sem_labels):       
        
        #output interme results
        has_prediction = pre_sub_ins != -1
        #print(np.any(has_prediction))
        if np.any(has_prediction):
            if not os.path.exists("viz"):
                    os.mkdir("viz")
            if hasattr(outputs, 'embed_logits'):
                val_name = join("viz", "block_sub_embed_"+str(self.block_count))
                embed_i = outputs.embed_logits.cpu().detach().numpy()
                sample_embed_logits = normalize(embed_i, axis=0)
                write_ply(val_name,
                            [self._test_area.pos[origin_sub_ids].detach().cpu().numpy(), 
                            pre_sub_ins.astype('int32'),
                            self._test_area.instance_labels[origin_sub_ids].detach().cpu().numpy().astype('int32'),
                            sample_embed_logits.astype('float32'),
                            predicted_sem_labels.cpu().numpy().astype('int32'),
                            self._test_area.y[origin_sub_ids].detach().cpu().numpy().astype('int32')
                            ],
                            ['x', 'y', 'z', 'preins_label','ins_gt','embed1','embed2','embed3','embed4','embed5',
                            #['x', 'y', 'z', 'preins_label','ins_gt','embed1','embed2','embed3','embed4','embed5','embed6','embed7','embed8',
                             'pre_sem_label', 'gt_sem_label'])
            if hasattr(outputs, 'offset_logits'):
                val_name = join("viz", "block_sub_offset_"+str(self.block_count))
                offset_i = outputs.offset_logits.cpu().detach().numpy()
                shifted_cor= offset_i+self._test_area.pos[origin_sub_ids].detach().cpu().numpy()
                write_ply(val_name,
                            [self._test_area.pos[origin_sub_ids].detach().cpu().numpy(), 
                            pre_sub_ins.astype('int32'),
                            self._test_area.instance_labels[origin_sub_ids].detach().cpu().numpy().astype('int32'),
                            shifted_cor.astype('float32'),
                            predicted_sem_labels.cpu().numpy().astype('int32'),
                            self._test_area.y[origin_sub_ids].detach().cpu().numpy().astype('int32')
                            ],
                            ['x', 'y', 'z', 'preins_label', 'ins_gt', 'center_pre_x','center_pre_y','center_pre_z',
                             'pre_sem_label', 'gt_sem_label'])
            
            #assign_index  = knn(self._test_area.pos[origin_sub_ids[has_prediction]], self._test_area.pos[originids], k=1)
            assign_index  = knn(self._test_area.pos[origin_sub_ids], self._test_area.pos[originids], k=1)
    
            y_idx, x_idx = assign_index
            
            #pre_ins = pre_sub_ins[has_prediction][x_idx.detach().cpu().numpy()]
            pre_ins = pre_sub_ins[x_idx.detach().cpu().numpy()]
            #has_prediction = full_ins_pred != -1
            val_name = join("viz", "block_"+str(self.block_count))
            write_ply(val_name,
                        [self._test_area.pos[originids].detach().cpu().numpy(), 
                        pre_ins.astype('int32'),
                        ],
                        ['x', 'y', 'z', 'preins_label'])
    
            t_num_clusters = np.max(pre_ins)+1
            #print(np.unique(pre_ins))
            #print(t_num_clusters)
            #print("t_num_clusters: {}".format(t_num_clusters))
            #print("max_instance: {}".format(max_instance))
            #print("th_merge: {}".format(th_merge))
            idx = np.argwhere(all_pre_ins[originids] != -1)  #has label
            idx2 = np.argwhere(all_pre_ins[originids] == -1) #no label
            
            #all points have no label
            if len(idx)==0: #t_num_clusters>0:    
                mask_valid = pre_ins != -1                     
                all_pre_ins[originids[mask_valid]] = pre_ins[mask_valid] + max_instance                     
                max_instance = max_instance + t_num_clusters
                #print("test")
                #return torch.from_numpy(all_pre_ins), max_instance
            #all points have labels
            elif len(idx2)==0: 
                return  torch.from_numpy(all_pre_ins), max_instance
            #part of points have labels
            else:                       
                #merge by iou
                new_label = pre_ins.reshape(-1)  
                    
                for ii_idx in range(t_num_clusters):   
                    new_label_ii_idx = originids[np.argwhere(new_label==ii_idx).reshape(-1)]
                        
                    new_has_old_idx = new_label_ii_idx[np.argwhere(all_pre_ins[new_label_ii_idx]!=-1)]  #new prediction already has old label
                    new_not_old_idx = new_label_ii_idx[np.argwhere(all_pre_ins[new_label_ii_idx]==-1)] #new prediction has no old label  
                    #print(new_has_old_idx)
                    #print(len(new_label_ii_idx))  
                    #print(len(new_has_old_idx))     
                    #print(len(new_not_old_idx))  
                    if len(new_has_old_idx)==0:
                        all_pre_ins[new_not_old_idx] = max_instance+1
                        max_instance = max_instance+1
                    elif len(new_not_old_idx)==0: 
                        continue
                    else:
                        old_labels_ii = all_pre_ins[new_has_old_idx]
                        un = np.unique(old_labels_ii)
                        #print(un)
                        max_iou_ii = 0
                        max_iou_ii_oldlabel = 0
                        for ig, g in enumerate(un):
                            idx_old_all = originids[np.argwhere(all_pre_ins[originids]==g).reshape(-1)]
                            union_label_idx = np.union1d(idx_old_all, new_label_ii_idx)
                            inter_label_idx = np.intersect1d(idx_old_all, new_label_ii_idx)
                            #print(inter_label_idx.size)
                            iou = float(inter_label_idx.size) / float(union_label_idx.size)
                            #print(iou)
                            if iou > max_iou_ii:
                                max_iou_ii = iou
                                max_iou_ii_oldlabel = g
                                
                        if max_iou_ii > 0.1: #th_merge:
                            all_pre_ins[new_not_old_idx] = max_iou_ii_oldlabel
                        else:
                            all_pre_ins[new_not_old_idx] = max_instance+1
                            max_instance = max_instance+1     
        return torch.from_numpy(all_pre_ins), max_instance
    
    def non_max_suppression(self, ious, scores, threshold):
        ixs = scores.argsort()[::-1]
        pick = []
        while len(ixs) > 0:
            i = ixs[0]
            pick.append(i)
            iou = ious[i, ixs[1:]]
            remove_ixs = np.where(iou > threshold)[0] + 1
            ixs = np.delete(ixs, remove_ixs)
            ixs = np.delete(ixs, 0)
        return pick
    
    def block_merging_by_score(self, all_scores, all_clusters, new_scores, new_clusters, originids, origin_sub_ids):
        if not new_clusters:
            return all_clusters, all_scores
        
        assign_index  = knn(self._test_area.pos[origin_sub_ids], self._test_area.pos[originids], k=1)
        y_idx, x_idx = assign_index
        #pre_ins = pre_sub_ins[has_prediction][x_idx.detach().cpu().numpy()]
        #pre_ins = pre_sub_ins[x_idx.detach().cpu().numpy()]
        new_clusters_backp = []
        for i, cluster in enumerate(new_clusters):
            indices = torch.zeros_like(originids, dtype = torch.uint8).to(cluster.device)
            for elem in cluster:
                indices = indices | (x_idx == elem)  
            new_clusters_backp.append(originids[indices].to(cluster.device))

        nms_threshold=0.3
        #min_cluster_points=100
        #min_score=0.2
        #print(len(all_clusters))
        all_prop_to_be_merged = all_clusters+new_clusters_backp
        if all_scores is None:
            all_prop_scores = new_scores
        else:
            all_prop_scores = torch.cat((all_scores, new_scores))
        
        '''n_prop = len(all_prop_to_be_merged)
        proposal_masks = torch.zeros(n_prop, self._test_area.y.shape[0]) #.to(cluster.device)
        # for i, cluster in enumerate(self.clusters):
        #     proposal_masks[i, cluster] = 1
        
        proposals_idx = []
        for i, cluster in enumerate(all_prop_to_be_merged):
            proposal_id = torch.ones(len(cluster)).cuda()*i
            proposals_idx.append(torch.vstack((proposal_id,cluster)).T)
        proposals_idx = torch.cat(proposals_idx, dim=0)
        proposals_idx_filtered = proposals_idx
        #proposals_idx_filtered = proposals_idx[_mask]
        proposal_masks[proposals_idx_filtered[:, 0].long(), proposals_idx_filtered[:, 1].long()] = 1

        intersection = torch.mm(proposal_masks, proposal_masks.t())  # (nProposal, nProposal), float, cuda
        proposals_pointnum = proposal_masks.sum(1)  # (nProposal), float, cuda
        proposals_pn_h = proposals_pointnum.unsqueeze(-1).repeat(1, proposals_pointnum.shape[0])
        proposals_pn_v = proposals_pointnum.unsqueeze(0).repeat(proposals_pointnum.shape[0], 1)
        cross_ious = intersection / (proposals_pn_h + proposals_pn_v - intersection)
        pick_idxs = self.non_max_suppression(cross_ious.cpu().numpy(), all_prop_scores.cpu().numpy(), nms_threshold)
        '''
        n_prop = len(all_prop_to_be_merged)
        cross_ious = np.identity(n_prop)
        for i, cluster_i in enumerate(all_prop_to_be_merged[:-2]):
            #for j, cluster_j in enumerate(all_prop_to_be_merged):
            t1=cluster_i.cpu().numpy()
            j=i+1
            t2=all_prop_to_be_merged[j].cpu().numpy() 
            intersection = np.intersect1d(t1, t2).shape[0]
            union = np.union1d(t1, t2).shape[0]
            iou = intersection/union
            cross_ious[i][j]=iou
            cross_ious[j][i]=iou
        pick_idxs = self.non_max_suppression(cross_ious, all_prop_scores.cpu().numpy(), nms_threshold)
        all_scores = []
        all_clusters = []
        for i in pick_idxs:
            #cl_mask = proposals_idx_filtered[:,0]==i
            #cl = proposals_idx_filtered[cl_mask][:,1].long()
            #if len(cl) > min_cluster_points and self.cluster_scores[i] > min_score:
            #all_scores.append(all_prop_scores[i])
            all_clusters.append(all_prop_to_be_merged[i])
        all_scores = all_prop_scores[pick_idxs]
        return all_clusters, all_scores

    def finalise(self, full_res=False, vote_miou=True, ply_output="", track_instances=True, **kwargs):
        per_class_iou = self._confusion_matrix.get_intersection_union_per_class()[0]
        self._iou_per_class = {k: v for k, v in enumerate(per_class_iou)}
        
        if vote_miou and self._test_area:
            # Complete for points that have a prediction
            self._test_area = self._test_area.to("cpu")
            c = ConfusionMatrix(self._num_classes)
            has_prediction = self._test_area.prediction_count > 0
            gt = self._test_area.y[has_prediction].numpy()
            pred = torch.argmax(self._test_area.votes[has_prediction], 1).numpy()
            gt_effect = gt >= 0
            c.count_predicted_batch(gt[gt_effect], pred[gt_effect])
            self._vote_miou = c.get_average_intersection_union() * 100

        if full_res:
            self._compute_full_miou()

        if ply_output:
            has_prediction = self._test_area.prediction_count > 0
            #semantic prediction with color for subsampled cloud
            self._dataset.to_ply(
                self._test_area.pos[has_prediction].cpu(),
                torch.argmax(self._test_area.votes[has_prediction], 1).cpu().numpy(),
                ply_output,
            )
            
            #self._test_area = self._test_area.to("cpu")
            full_pred = knn_interpolate(
            self._test_area.votes[has_prediction], self._test_area.pos[has_prediction], self._test_area.pos, k=1,
            )
            #semantic prediction with color for full cloud
            '''self._dataset.to_ply(
                self._test_area.pos,
                torch.argmax(full_pred, 1).numpy(),
                "vote1regularfull.ply",
            )'''
            #semantic prediction and GT label full cloud (for final evaluation)
            #self._dataset.to_eval_ply(
            #    self._test_area.pos,
            #    torch.argmax(full_pred, 1).numpy(), #[0, ..]
            #    self._test_area.y,   #[-1, ...]
            #    "Semantic_results_forEval.ply",
            #)
            #instance
            has_prediction = self._test_area.ins_pre != -1
            #full_ins_pred_embed = knn_interpolate(
            #torch.reshape(self._test_area.ins_pre_embed[has_prediction], (-1,1)), self._test_area.pos[has_prediction], self._test_area.pos, k=1,
            #)
            
            #instance prediction with color for subsampled cloud
            self._dataset.to_ins_ply(
                self._test_area.pos[has_prediction].cpu(),
                self._test_area.ins_pre[has_prediction].cpu().numpy(),
                "Instance_subsample.ply",
            )
            
            assign_index  = knn(self._test_area.pos[has_prediction], self._test_area.pos, k=1)

            #assign_index2  = nearest(self._test_area.pos, self._test_area.pos[has_prediction])

            y_idx, x_idx = assign_index
            
            full_ins_pred = self._test_area.ins_pre[has_prediction][x_idx]

            full_ins_pred = torch.reshape(full_ins_pred, (-1,))
            #instance prediction and GT label full cloud (for final evaluation)
            
            #idx_in_cur = [idx for idx, l in enumerate(torch.argmax(full_pred, 1).numpy()) if l in self._dataset.stuff_classes]
            #idx_in_cur = np.array(idx_in_cur)
            #idx_in_cur.astype(int)
            
            # assign no instance label for points belonging to stuff classes
            pre_sem_labels_full = torch.argmax(full_pred, 1)
            for idx, l in enumerate(self._dataset.stuff_classes):
                idx_in_cur = (pre_sem_labels_full == l).nonzero(as_tuple=True)[0].numpy().astype(int)
                full_ins_pred[idx_in_cur] = -1
            
            #If the distance between the point to be assigned label and its nearest point (the point which already has point) is larger than a threshold (e.g., 1m), assign -1 to this point as well.
            mat_pos = torch.sub(self._test_area.pos[has_prediction][x_idx], self._test_area.pos[y_idx])
            mat_pos = mat_pos.pow(2)
            mat_pos = torch.sum(mat_pos, 1)
            mat_pos = mat_pos.sqrt()
            idx_in_cur = mat_pos>1
            full_ins_pred[idx_in_cur] = -1
            #for i,k in enumerate(x_idx):
            #    if (self._test_area.pos[has_prediction][k]-self._test_area.pos[i]).pow(2).sum().sqrt()>1:
            #        full_ins_pred[i]=-1
            
            #remove instance that contains point number less than 10
            unique_predicted_inslabels = torch.unique(full_ins_pred)
            for l in unique_predicted_inslabels:
                if l==-1:
                    continue
                label_mask_l = full_ins_pred == l
                size_l = full_ins_pred[label_mask_l].shape[0]
                if size_l<10:
                    full_ins_pred[label_mask_l] = -1
            
            '''self._dataset.to_eval_ply(
                self._test_area.pos,
                full_ins_pred_embed.numpy(),  #[-1, ...]
                self._test_area.instance_labels,  #[0, ..]
                "Instance_Embed_results_forEval.ply",
            )
            self._dataset.to_eval_ply(
                self._test_area.pos,
                full_ins_pred_offset.numpy(),
                self._test_area.instance_labels,
                "Instance_Offset_results_forEval.ply",
            )'''
            
            self._dataset.final_eval(
                torch.argmax(full_pred, 1).numpy(),
                full_ins_pred.numpy(),
                full_ins_pred.numpy(),
                self._test_area.pos,
                self._test_area.y,
                self._test_area.instance_labels,
            )
            #instance prediction with color for "things"
            things_idx = full_ins_pred != -1
            self._dataset.to_ins_ply(
                self._test_area.pos[things_idx],
                full_ins_pred[things_idx].numpy(),
                "Instance_results_withColor.ply",
            )

        if not track_instances:
            return

        rec, _, ap = self._ap_meter.eval(self._iou_threshold)
        self._ap = OrderedDict(sorted(ap.items()))
        self._rec = OrderedDict({})
        for key, val in sorted(rec.items()):
            try:
                value = val[-1]
            except TypeError:
                value = val
            self._rec[key] = value

    @staticmethod
    def _compute_acc(clusters, predicted_labels, labels, batch, num_instances, iou_threshold):
        """ Computes the ratio of True positives, False positives and accuracy
        """
        iou_values, gt_ids = instance_iou(clusters, labels.instance_labels, batch).max(1)
        gt_ids += 1
        instance_offsets = torch.cat((torch.tensor([0]).to(num_instances.device), num_instances.cumsum(-1)))
        tp = 0
        fp = 0
        for i, iou in enumerate(iou_values):
            # Too low iou, no match in ground truth
            if iou < iou_threshold:
                fp += 1
                continue

            # Check that semantic is correct
            sample_idx = batch[clusters[i][0]]
            sample_mask = batch == sample_idx
            instance_offset = instance_offsets[sample_idx]
            gt_mask = labels.instance_labels[sample_mask] == (gt_ids[i] - instance_offset)
            gt_classes = labels.y[sample_mask][torch.nonzero(gt_mask, as_tuple=False)]
            gt_classes, counts = torch.unique(gt_classes, return_counts=True)
            gt_class = gt_classes[counts.max(-1)[1]]
            pred_class = torch.mode(predicted_labels[clusters[i]])
            #predicted_labels[clusters[i][0]]
            if gt_class == pred_class[0]:
                tp += 1
            else:
                fp += 1
        acc = tp / len(clusters)
        tp = tp / torch.sum(labels.num_instances).cpu().item()
        fp = fp / torch.sum(labels.num_instances).cpu().item()
        return tp, fp, acc

    @staticmethod
    def _compute_eval(clusters, predicted_labels, labels, batch, num_instances, num_classe, iou_threshold):
        
        ins_classcount_have = []
        pts_in_pred = [[] for itmp in range(num_classe)]
        for g in clusters:  # each object in prediction
            tmp = torch.zeros_like(predicted_labels, dtype=torch.bool)
            tmp[g] = True
            sem_seg_i = int(torch.mode(predicted_labels[tmp])[0])
            pts_in_pred[sem_seg_i] += [tmp]
            
        pts_in_gt = [[] for itmp in range(num_classe)]
        unique_in_batch = torch.unique(batch)
        for s in unique_in_batch:
            batch_mask = batch == s
            un = torch.unique(labels.instance_labels[batch_mask])
            for ig, g in enumerate(un):
                if g < 0:
                    continue
                tmp = (labels.instance_labels == g) & batch_mask.to(num_instances.device)
                sem_seg_i = int(torch.mode(labels.y[tmp])[0])
                if sem_seg_i == -1:
                    continue
                pts_in_gt[sem_seg_i] += [tmp]
                ins_classcount_have.append(sem_seg_i)
            
        all_mean_cov = [[] for itmp in range(num_classe)]
        all_mean_weighted_cov = [[] for itmp in range(num_classe)]
        # instance mucov & mwcov
        for i_sem in range(num_classe):
            sum_cov = 0
            mean_cov = 0
            mean_weighted_cov = 0
            num_gt_point = 0
            if not pts_in_gt[i_sem] or not pts_in_pred[i_sem]:
                all_mean_cov[i_sem].append(0)
                all_mean_weighted_cov[i_sem].append(0)
                continue
            for ig, ins_gt in enumerate(pts_in_gt[i_sem]):
                ovmax = 0.
                num_ins_gt_point = torch.sum(ins_gt)
                num_gt_point += num_ins_gt_point
                for ip, ins_pred in enumerate(pts_in_pred[i_sem]):
                    union = (ins_pred | ins_gt)
                    intersect = (ins_pred & ins_gt)
                    iou = float(torch.sum(intersect)) / torch.sum(union)

                    if iou >= ovmax:
                        ovmax = iou
                        ipmax = ip

                sum_cov += ovmax
                mean_weighted_cov += ovmax * num_ins_gt_point

            if len(pts_in_gt[i_sem]) != 0:
                mean_cov = sum_cov / len(pts_in_gt[i_sem])
                all_mean_cov[i_sem].append(mean_cov.item())

                mean_weighted_cov /= num_gt_point
                all_mean_weighted_cov[i_sem].append(mean_weighted_cov.item())

        #print(all_mean_cov)
        total_gt_ins = np.zeros(num_classe)
        at = iou_threshold
        tpsins = [[] for itmp in range(num_classe)]
        fpsins = [[] for itmp in range(num_classe)]
        IoU_Tp = np.zeros(num_classe)
        IoU_Mc = np.zeros(num_classe)
        # instance precision & recall
        for i_sem in range(num_classe):
            if not pts_in_pred[i_sem]:
                continue
            IoU_Tp_per=0
            IoU_Mc_per=0
            tp = [0.] * len(pts_in_pred[i_sem])
            fp = [0.] * len(pts_in_pred[i_sem])
            #gtflag = np.zeros(len(pts_in_gt[i_sem]))
            if pts_in_gt[i_sem]:
                total_gt_ins[i_sem] += len(pts_in_gt[i_sem])
            for ip, ins_pred in enumerate(pts_in_pred[i_sem]):
                ovmax = -1.
                if not pts_in_gt[i_sem]:
                    fp[ip] = 1
                    continue
                for ig, ins_gt in enumerate(pts_in_gt[i_sem]):
                    union = (ins_pred | ins_gt)
                    intersect = (ins_pred & ins_gt)
                    iou = (float(torch.sum(intersect)) / torch.sum(union)).item()

                    if iou > ovmax:
                        ovmax = iou
                        #igmax = ig

                if ovmax > 0:
                    IoU_Mc_per += ovmax
                if ovmax >= at:
                    tp[ip] = 1  # true
                    IoU_Tp_per += ovmax
                else:
                    fp[ip] = 1  # false positive

            tpsins[i_sem] += tp
            fpsins[i_sem] += fp
            IoU_Tp[i_sem] += IoU_Tp_per
            IoU_Mc[i_sem] += IoU_Mc_per
        
        MUCov = torch.zeros(num_classe)
        MWCov = torch.zeros(num_classe)

        for i_sem in range(num_classe):
            MUCov[i_sem] = np.mean(all_mean_cov[i_sem])
            MWCov[i_sem] = np.mean(all_mean_weighted_cov[i_sem])
        
        precision = torch.zeros(num_classe)
        recall = torch.zeros(num_classe)
        RQ = torch.zeros(num_classe)
        SQ = torch.zeros(num_classe)
        PQ = torch.zeros(num_classe)
        PQStar = torch.zeros(num_classe)
        ins_classcount = [2,3,4,6,7,8] 
        set1 = set(ins_classcount)
        set2 = set(ins_classcount_have)
        set3 = set1 & set2
        list3 = list(set3)
        ################################################################
        ######  recall, precision, RQ, SQ, PQ, PQ_star for things ###### 
        ################################################################
        for i_sem in ins_classcount:
            ###### metrics for offset ######
            if not tpsins[i_sem] or not fpsins[i_sem]:
                continue
            tp = np.asarray(tpsins[i_sem]).astype(np.float)
            fp = np.asarray(fpsins[i_sem]).astype(np.float)
            tp = np.sum(tp)
            fp = np.sum(fp)
            # recall and precision
            if (total_gt_ins[i_sem])==0:
                rec = 0
            else:
                rec = tp / total_gt_ins[i_sem]
            if (tp + fp)==0:
                prec = 0
            else:
                prec = tp / (tp + fp)
            precision[i_sem] = prec
            recall[i_sem] = rec
            # RQ, SQ, PQ and PQ_star
            if (prec+rec)==0:
                RQ[i_sem] = 0
            else:
                RQ[i_sem] = 2*prec*rec/(prec+rec)
            if tp==0:
                SQ[i_sem] = 0
            else:
                SQ[i_sem] = IoU_Tp[i_sem]/tp
            PQ[i_sem] = SQ[i_sem]*RQ[i_sem]
            # PQStar[i_sem] = IoU_Mc[i_sem]/total_gt_ins[i_sem]
            PQStar[i_sem] = PQ[i_sem]
        
        if torch.mean(precision[list3])+torch.mean(recall[list3])==0:
            F1_score = torch.tensor(0.)
        else:
            F1_score = (2*torch.mean(precision[list3])*torch.mean(recall[list3]))/(torch.mean(precision[list3])+torch.mean(recall[list3]))
        cov = torch.mean(MUCov[list3])
        wcov = torch.mean(MWCov[list3])
        mPre = torch.mean(precision[list3])
        mRec = torch.mean(recall[list3])

        return cov, wcov, mPre, mRec, F1_score
            
    def _compute_full_miou(self):
        if self._full_vote_miou is not None:
            return

        has_prediction = self._test_area.prediction_count > 0
        log.info(
            "Computing full res mIoU, we have predictions for %.2f%% of the points."
            % (torch.sum(has_prediction) / (1.0 * has_prediction.shape[0]) * 100)
        )

        self._test_area = self._test_area.to("cpu")

        # Full res interpolation
        full_pred = knn_interpolate(
            self._test_area.votes[has_prediction], self._test_area.pos[has_prediction], self._test_area.pos, k=1,
        )

        # Full res pred
        self._full_confusion = ConfusionMatrix(self._num_classes)
        gt_effect = self._test_area.y >= 0
        self._full_confusion.count_predicted_batch(self._test_area.y[gt_effect].numpy(), torch.argmax(full_pred, 1)[gt_effect].numpy())
        self._full_vote_miou = self._full_confusion.get_average_intersection_union() * 100

    @staticmethod
    def _extract_clusters(outputs, min_cluster_points):
        valid_cluster_idx, clusters = outputs.get_instances(min_cluster_points=min_cluster_points)
        # clusters = [outputs.clusters[i] for i in valid_cluster_idx]
        return clusters, valid_cluster_idx

    @staticmethod
    def _pred_instances_per_scan(clusters, predicted_labels, scores, batch, scan_id_offset):
        # Get sample index offset
        ones = torch.ones_like(batch)
        sample_sizes = torch.cat((torch.tensor([0]).to(batch.device), scatter_add(ones, batch)))
        offsets = sample_sizes.cumsum(dim=-1).cpu().numpy()

        # Build instance objects
        instances = []
        for i, cl in enumerate(clusters):
            sample_idx = batch[cl[0]].item()
            scan_id = sample_idx + scan_id_offset
            indices = cl.cpu().numpy() - offsets[sample_idx]
            if scores==None:
                instances.append(
                    _Instance(
                        classname=predicted_labels[cl[0]].item(), score=-1, indices=indices, scan_id=scan_id
                    )
                )
            else:
                instances.append(
                    _Instance(
                        classname=predicted_labels[cl[0]].item(), score=scores[i].item(), indices=indices, scan_id=scan_id
                    )
                )
        return instances

    @staticmethod
    def _gt_instances_per_scan(instance_labels, gt_labels, batch, scan_id_offset):
        batch_size = batch[-1] + 1
        instances = []
        for b in range(batch_size):
            sample_mask = batch == b
            instances_in_sample = instance_labels[sample_mask]
            gt_labels_sample = gt_labels[sample_mask]
            num_instances = torch.max(instances_in_sample)
            scan_id = b + scan_id_offset
            for i in range(num_instances):
                instance_indices = torch.where(instances_in_sample == i + 1)[0].cpu().numpy()
                instances.append(
                    _Instance(
                        classname=gt_labels_sample[instance_indices[0]].item(),
                        score=-1,
                        indices=instance_indices,
                        scan_id=scan_id,
                    )
                )
        return instances

    def get_metrics(self, verbose=False) -> Dict[str, Any]:
        """ Returns a dictionnary of all metrics and losses being tracked
        """
        metrics = super().get_metrics(verbose)

        metrics["{}_pos".format(self._stage)] = meter_value(self._pos)
        metrics["{}_neg".format(self._stage)] = meter_value(self._neg)
        metrics["{}_Iacc".format(self._stage)] = meter_value(self._acc_meter)
        
        metrics["{}_cov".format(self._stage)] = meter_value(self._cov)
        metrics["{}_wcov".format(self._stage)] = meter_value(self._wcov)
        metrics["{}_mIPre".format(self._stage)] = meter_value(self._mIPre)
        metrics["{}_mIRec".format(self._stage)] = meter_value(self._mIRec)
        metrics["{}_F1".format(self._stage)] = meter_value(self._F1)

        if self._has_instance_data:
            mAP = sum(self._ap.values()) / len(self._ap)
            metrics["{}_map".format(self._stage)] = mAP

        if verbose:
            metrics["{}_iou_per_class".format(self._stage)] = self._iou_per_class

        if verbose and self._has_instance_data:
            metrics["{}_class_rec".format(self._stage)] = self._dict_to_str(self._rec)
            metrics["{}_class_ap".format(self._stage)] = self._dict_to_str(self._ap)
        return metrics

    @property
    def _has_instance_data(self):
        return len(self._rec)
